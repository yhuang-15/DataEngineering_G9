{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8115076-f209-43e4-b6a7-24b9e5139890",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datetime\n",
      "  Downloading DateTime-4.3-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 3.7 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pytz in /opt/conda/lib/python3.9/site-packages (from datetime) (2021.1)\n",
      "Collecting zope.interface\n",
      "  Downloading zope.interface-5.4.0-cp39-cp39-manylinux2010_x86_64.whl (255 kB)\n",
      "\u001b[K     |████████████████████████████████| 255 kB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from zope.interface->datetime) (49.6.0.post20210108)\n",
      "Installing collected packages: zope.interface, datetime\n",
      "Successfully installed datetime-4.3 zope.interface-5.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75660944-79cb-43cc-827c-e6e4ebde6c85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+---+-----+--------+\n",
      "|start_time|end_time|gender|amt|count|category|\n",
      "+----------+--------+------+---+-----+--------+\n",
      "+----------+--------+------+---+-----+--------+\n",
      "\n",
      "+----------+--------+------+---+-----+--------+\n",
      "|start_time|end_time|gender|amt|count|category|\n",
      "+----------+--------+------+---+-----+--------+\n",
      "+----------+--------+------+---+-----+--------+\n",
      "\n",
      "+----------+--------+------+---+-----+--------+\n",
      "|start_time|end_time|gender|amt|count|category|\n",
      "+----------+--------+------+---+-----+--------+\n",
      "+----------+--------+------+---+-----+--------+\n",
      "\n",
      "+----------+--------+------+---+-----+--------+\n",
      "|start_time|end_time|gender|amt|count|category|\n",
      "+----------+--------+------+---+-----+--------+\n",
      "+----------+--------+------+---+-----+--------+\n",
      "\n",
      "+----------+--------+------+---+-----+--------+\n",
      "|start_time|end_time|gender|amt|count|category|\n",
      "+----------+--------+------+---+-----+--------+\n",
      "+----------+--------+------+---+-----+--------+\n",
      "\n",
      "+----------+--------+------+---+-----+--------+\n",
      "|start_time|end_time|gender|amt|count|category|\n",
      "+----------+--------+------+---+-----+--------+\n",
      "+----------+--------+------+---+-----+--------+\n",
      "\n",
      "+-------------------+-------------------+------+------+-----+-------------+\n",
      "|start_time         |end_time           |gender|amt   |count|category     |\n",
      "+-------------------+-------------------+------+------+-----+-------------+\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|F     |94.63 |1    |gas_transport|\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|M     |220.11|1    |entertainment|\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|F     |4.97  |1    |misc_net     |\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|M     |41.96 |1    |misc_pos     |\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|M     |45.0  |1    |gas_transport|\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|F     |107.23|1    |grocery_pos  |\n",
      "+-------------------+-------------------+------+------+-----+-------------+\n",
      "\n",
      "+-------------------+-------------------+------+------+-----+-------------+\n",
      "|start_time         |end_time           |gender|amt   |count|category     |\n",
      "+-------------------+-------------------+------+------+-----+-------------+\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|F     |94.63 |1    |gas_transport|\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|M     |220.11|1    |entertainment|\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|F     |4.97  |1    |misc_net     |\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|M     |41.96 |1    |misc_pos     |\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|M     |45.0  |1    |gas_transport|\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|F     |107.23|1    |grocery_pos  |\n",
      "|2019-01-01 00:05:00|2019-01-01 00:10:00|F     |198.39|1    |grocery_pos  |\n",
      "|2019-01-01 00:05:00|2019-01-01 00:10:00|M     |192.25|3    |grocery_pos  |\n",
      "|2019-01-01 00:05:00|2019-01-01 00:10:00|F     |7.77  |1    |shopping_net |\n",
      "+-------------------+-------------------+------+------+-----+-------------+\n",
      "\n",
      "+-------------------+-------------------+------+------+-----+-------------+\n",
      "|start_time         |end_time           |gender|amt   |count|category     |\n",
      "+-------------------+-------------------+------+------+-----+-------------+\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|F     |94.63 |1    |gas_transport|\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|M     |220.11|1    |entertainment|\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|F     |4.97  |1    |misc_net     |\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|M     |41.96 |1    |misc_pos     |\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|M     |45.0  |1    |gas_transport|\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|F     |107.23|1    |grocery_pos  |\n",
      "|2019-01-01 00:05:00|2019-01-01 00:10:00|F     |198.39|1    |grocery_pos  |\n",
      "|2019-01-01 00:05:00|2019-01-01 00:10:00|M     |192.25|3    |grocery_pos  |\n",
      "|2019-01-01 00:05:00|2019-01-01 00:10:00|F     |7.77  |1    |shopping_net |\n",
      "+-------------------+-------------------+------+------+-----+-------------+\n",
      "\n",
      "+-------------------+-------------------+------+------+-----+-------------+\n",
      "|start_time         |end_time           |gender|amt   |count|category     |\n",
      "+-------------------+-------------------+------+------+-----+-------------+\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|F     |94.63 |1    |gas_transport|\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|M     |220.11|1    |entertainment|\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|F     |4.97  |1    |misc_net     |\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|M     |41.96 |1    |misc_pos     |\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|M     |45.0  |1    |gas_transport|\n",
      "|2019-01-01 00:00:00|2019-01-01 00:05:00|F     |107.23|1    |grocery_pos  |\n",
      "|2019-01-01 00:05:00|2019-01-01 00:10:00|F     |198.39|1    |grocery_pos  |\n",
      "|2019-01-01 00:05:00|2019-01-01 00:10:00|M     |192.25|3    |grocery_pos  |\n",
      "|2019-01-01 00:05:00|2019-01-01 00:10:00|F     |7.77  |1    |shopping_net |\n",
      "+-------------------+-------------------+------+------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, concat, col, lit, mean, window, sum, count, round, max\n",
    "from pyspark.sql.window import Window\n",
    "from time import sleep\n",
    "from datetime import datetime \n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Assignment_2\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "date = datetime.now().strftime(\"%m%d%M\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "# Read the whole dataset as a batch\n",
    "df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "        .option(\"subscribe\", f\"records{date}\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"failOnDataLoss\", 'False') \\\n",
    "        .load()\n",
    "\n",
    "# split the value\n",
    "records = df.selectExpr(\"CAST(value AS STRING)\").select(split(col('value'), \",\").alias('splitted'))\n",
    "\n",
    "# define the schema in this way\n",
    "new_df =  records.selectExpr('splitted[0] as id', \n",
    "                             'cast(splitted[1] as timestamp) as event_time',\n",
    "                             'cast(splitted[2] as long) as cc_num',\n",
    "                             'splitted[3] as merchant',\n",
    "                             'splitted[4] as category',\n",
    "                             'cast(splitted[5] as double) as amt',\n",
    "                             'splitted[6] as first',\n",
    "                             'splitted[7] as last',\n",
    "                             'splitted[8] as gender',\n",
    "                             'splitted[9] as street',\n",
    "                             'splitted[10] as city',\n",
    "                             'splitted[11] as state',\n",
    "                             'splitted[12] as zip',\n",
    "                             'splitted[13] as lat',\n",
    "                             'splitted[14] as long',\n",
    "                             'cast(splitted[15] as long) as city_pop',\n",
    "                             'splitted[16] as job',\n",
    "                             'splitted[17] as dob',\n",
    "                             'splitted[18] as trans_num',\n",
    "                             'cast(splitted[19] as long) as unix_time',\n",
    "                             'splitted[20] as merch_lat',\n",
    "                             'splitted[21] as merch_long',\n",
    "                             'cast(splitted[22] as int) as is_fraud')\n",
    "\n",
    "new_df = new_df.select('event_time', 'category', 'amt', 'gender', 'is_fraud')\n",
    "\n",
    "# filter out the fraud\n",
    "#new_df = new_df.where(col('is_fraud') == 1)\n",
    "\n",
    "window_5min = window(col('event_time'), '5 minutes').alias('time_slot')\n",
    "sdf = new_df.groupBy(window_5min, 'gender', 'category') \\\n",
    "            .agg(sum('amt').alias('amt'), count('*').alias('count'))\n",
    "\n",
    "window_group_by = Window.partitionBy(window_5min, col('gender'))\n",
    "sdf = sdf.withColumn('max_amt_per_5_min', max('amt').over(window_group_by)) \\\n",
    "         .where(col('amt') == col('max_amt_per_5_min'))\\\n",
    "         .drop('max_amt_per_5_min')\n",
    "            \n",
    "# Write to a sink - here, the output is memory (only for testing). \n",
    "activityQuery = sdf.select(sdf.time_slot.start.cast('string').alias('start_time'), \n",
    "                           sdf.time_slot.end.cast('string').alias('end_time'), \n",
    "                           'gender', round('amt', 2).alias('amt'), 'count', 'category') \\\n",
    "                   .writeStream \\\n",
    "                   .queryName(\"most_fraud_table\") \\\n",
    "                   .format(\"memory\").outputMode(\"update\") \\\n",
    "                   .start()\n",
    "# Testing \n",
    "for x in range(10):\n",
    "    spark.sql(\"SELECT * FROM most_fraud_table\").show(truncate=False)\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acdad09-753c-485c-8c87-b303392053ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = string_sdf_User_Team_avg_window \\\n",
    "              .select(concat(col(\"window_slot\"), lit(\",\"), col(\"User\"), lit(\",\"), col(\"Team\")).alias(\"key\"), \n",
    "                       col(\"avg_score\").alias('value') ) \\\n",
    "              .writeStream \\\n",
    "              .format(\"kafka\") \\\n",
    "              .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "              .option(\"checkpointLocation\", \"/home/jovyan/checkpoint\")\\\n",
    "              .option(\"topic\", \"avg_score\") \\\n",
    "              .outputMode(\"complete\") \\\n",
    "              .start()\n",
    "\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af9584fd-4cf0-4caa-a571-ec671dbd3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0346deed-8f8c-45bf-a8ca-8f4ac432669d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, TimestampType\n",
    "from time import sleep\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Lab7_exercise\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "dataSchema = StructType(\n",
    "    [StructField(\"User\", StringType(), True),\n",
    "     StructField(\"Team\", StringType(), True),\n",
    "     StructField(\"Score\", DoubleType(), True),\n",
    "     StructField(\"timestamp_in_ms\", LongType(), True),\n",
    "     StructField(\"event_time\", TimestampType(), True)\n",
    "     ])\n",
    "\n",
    "df = spark.read.format('csv').schema(dataSchema).option('header', 'False').load(\"/home/jovyan/data/gamestream/game_data_split1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83aca9fc-8e73-4600-b8b4-48859ef63500",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Score: double (nullable = true)\n",
      " |-- timestamp_in_ms: long (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n",
      "+--------------------------+--------------------+-----+---------------+-----------------------+\n",
      "|User                      |Team                |Score|timestamp_in_ms|event_time             |\n",
      "+--------------------------+--------------------+-----+---------------+-----------------------+\n",
      "|user16_AmaranthKoala      |AmaranthKoala       |18.0 |1447719060000  |2015-11-16 16:11:03.921|\n",
      "|user10_AndroidGreenKoala  |AndroidGreenKoala   |2.0  |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user9_AuburnCockatoo      |AuburnCockatoo      |5.0  |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user1_AntiqueBrassPlatypus|AntiqueBrassPlatypus|7.0  |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user9_BattleshipGreyPossum|BattleshipGreyPossum|14.0 |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user1_AmaranthDingo       |AmaranthDingo       |14.0 |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user10_ApricotDingo       |ApricotDingo        |16.0 |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user0_AzureBilby          |AzureBilby          |8.0  |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user9_AzureBilby          |AzureBilby          |5.0  |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user12_AmaranthMarmot     |AmaranthMarmot      |16.0 |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user15_AmaranthKoala      |AmaranthKoala       |7.0  |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user18_AndroidGreenKoala  |AndroidGreenKoala   |5.0  |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user7_BananaWallaby       |BananaWallaby       |15.0 |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user11_AmaranthKoala      |AmaranthKoala       |16.0 |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user17_AuburnDingo        |AuburnDingo         |12.0 |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user14_AzureBilby         |AzureBilby          |12.0 |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user5_AzureBilby          |AzureBilby          |18.0 |1447719060000  |2015-11-16 16:11:03.96 |\n",
      "|user17_ApricotDingo       |ApricotDingo        |19.0 |1447719060000  |2015-11-16 16:11:03.96 |\n",
      "|user4_AuburnCockatoo      |AuburnCockatoo      |2.0  |1447719060000  |2015-11-16 16:11:03.96 |\n",
      "|user9_AmaranthKoala       |AmaranthKoala       |5.0  |1447719060000  |2015-11-16 16:11:03.96 |\n",
      "+--------------------------+--------------------+-----+---------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
