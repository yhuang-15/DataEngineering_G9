{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8115076-f209-43e4-b6a7-24b9e5139890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datetime\n",
      "  Downloading DateTime-4.3-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 3.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting zope.interface\n",
      "  Downloading zope.interface-5.4.0-cp39-cp39-manylinux2010_x86_64.whl (255 kB)\n",
      "\u001b[K     |████████████████████████████████| 255 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz in /opt/conda/lib/python3.9/site-packages (from datetime) (2021.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from zope.interface->datetime) (49.6.0.post20210108)\n",
      "Installing collected packages: zope.interface, datetime\n",
      "Successfully installed datetime-4.3 zope.interface-5.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75660944-79cb-43cc-827c-e6e4ebde6c85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoped the streaming query and the spark context\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, lit, window, sum, count, round, max, avg, broadcast\n",
    "from pyspark.sql.window import Window\n",
    "from time import sleep\n",
    "from datetime import datetime \n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Assignment_2\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "date = datetime.now().strftime(\"%m%d%M\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"dejads_temp_yk\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# Read the whole dataset as a batch\n",
    "df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "        .option(\"subscribe\", \"records\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"failOnDataLoss\", 'False') \\\n",
    "        .load()\n",
    "\n",
    "# split the value\n",
    "records = df.selectExpr(\"CAST(value AS STRING)\").select(split(col('value'), \",\").alias('splitted'))\n",
    "\n",
    "# define the schema in this way\n",
    "new_df =  records.selectExpr('splitted[0] as id', \n",
    "                             'cast(splitted[1] as timestamp) as event_time',\n",
    "                             'cast(splitted[2] as long) as cc_num',\n",
    "                             'splitted[3] as merchant',\n",
    "                             'splitted[4] as category',\n",
    "                             'cast(splitted[5] as double) as amt',\n",
    "                             'splitted[6] as first',\n",
    "                             'splitted[7] as last',\n",
    "                             'splitted[8] as gender',\n",
    "                             'splitted[9] as street',\n",
    "                             'splitted[10] as city',\n",
    "                             'splitted[11] as state',\n",
    "                             'splitted[12] as zip',\n",
    "                             'splitted[13] as lat',\n",
    "                             'splitted[14] as long',\n",
    "                             'cast(splitted[15] as long) as city_pop',\n",
    "                             'splitted[16] as job',\n",
    "                             'splitted[17] as dob',\n",
    "                             'splitted[18] as trans_num',\n",
    "                             'cast(splitted[19] as long) as unix_time',\n",
    "                             'splitted[20] as merch_lat',\n",
    "                             'splitted[21] as merch_long',\n",
    "                             'cast(splitted[22] as int) as is_fraud')\n",
    "\n",
    "new_df = new_df.select('event_time', 'category', 'amt', 'gender', 'is_fraud')\n",
    "\n",
    "# filter out the fraud\n",
    "#new_df = new_df.where(col('is_fraud') == 1)\n",
    "\n",
    "window_5min = window(col('event_time'), '5 minutes').alias('time_slot')\n",
    "sdf = new_df.groupBy(window_5min, 'gender', 'category') \\\n",
    "            .agg(sum('amt').alias('amt'), count('*').alias('count'))\n",
    "\n",
    "def defined_for_each_batch_function(df, epoch_id):\n",
    "    temp_df = df.groupBy(col('time_slot').alias('t_time_slot'), \n",
    "                      col(\"gender\").alias(\"t_gender\"))\\\n",
    "             .agg(max('amt').alias('max_amt'))\n",
    "    join_exp = (df['time_slot'] == temp_df['t_time_slot']) & \\\n",
    "               (df['gender'] == temp_df['t_gender']) & \\\n",
    "               (df['amt'] == temp_df['max_amt'])\n",
    "    df = df.join(broadcast(temp_df), join_exp)\\\n",
    "         .drop('t_gender')\\\n",
    "         .drop('t_time_slot')\\\n",
    "         .drop('max_amt')\n",
    "    \n",
    "    df.select('time_slot', 'gender', 'category',\n",
    "              round('amt', 2).alias('amt'), 'count') \\\n",
    "      .writeStream \\\n",
    "      .queryName(\"most_fraud_table\") \\\n",
    "      .format(\"memory\").outputMode(\"complete\") \\\n",
    "      .start()\n",
    "    \n",
    "    spark.sql(\"SELECT * FROM most_fraud_table\").show(truncate=False)\n",
    "    \n",
    "    df = df.select(df.time_slot.start.cast('string').alias('start_time'), \n",
    "                           df.time_slot.end.cast('string').alias('end_time'), \n",
    "                           'gender', 'category',\n",
    "                           'amt', 'count')\n",
    "    \n",
    "    # Saving the data to BigQuery\n",
    "    df.write.format('bigquery') \\\n",
    "       .option('table', 'jads-de-2021.assignment_2.streaming_table') \\\n",
    "       .mode(\"overwrite\") \\\n",
    "       .save()\n",
    "            \n",
    "# Write to a sink - here, the output is memory (only for testing). \n",
    "activityQuery = sdf.select('time_slot', \n",
    "                           'gender', 'category',\n",
    "                           round('amt', 2).alias('amt'), 'count') \\\n",
    "                   .writeStream \\\n",
    "                   .foreachBatch(defined_for_each_batch_function) \\\n",
    "                   .outputMode(\"complete\") \\\n",
    "                   .start()\n",
    "\n",
    "try:\n",
    "    activityQuery.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    activityQuery.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af9584fd-4cf0-4caa-a571-ec671dbd3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
