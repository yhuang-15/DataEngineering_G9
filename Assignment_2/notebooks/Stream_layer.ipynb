{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8115076-f209-43e4-b6a7-24b9e5139890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datetime\n",
      "  Downloading DateTime-4.3-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 3.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting zope.interface\n",
      "  Downloading zope.interface-5.4.0-cp39-cp39-manylinux2010_x86_64.whl (255 kB)\n",
      "\u001b[K     |████████████████████████████████| 255 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz in /opt/conda/lib/python3.9/site-packages (from datetime) (2021.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from zope.interface->datetime) (49.6.0.post20210108)\n",
      "Installing collected packages: zope.interface, datetime\n",
      "Successfully installed datetime-4.3 zope.interface-5.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75660944-79cb-43cc-827c-e6e4ebde6c85",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Multiple streaming aggregations are not supported with streaming DataFrames/Datasets;\nProject [cast(time_slot#7640.start as string) AS start_time#7702, cast(time_slot#7640.end as string) AS end_time#7703, gender#7596, category#7592, round(amt#7647, 2) AS amt#7704, count#7649L]\n+- Project [time_slot#7640, gender#7596, category#7592, amt#7647, count#7649L]\n   +- Project [time_slot#7640, gender#7596, category#7592, amt#7647, count#7649L, max_amt#7664]\n      +- Project [time_slot#7640, gender#7596, category#7592, amt#7647, count#7649L, t_time_slot#7656, max_amt#7664]\n         +- Join Inner, (((time_slot#7640 = t_time_slot#7656) AND (gender#7596 = t_gender#7657)) AND (amt#7647 = max_amt#7664))\n            :- Aggregate [window#7650, gender#7596, category#7592], [window#7650 AS time_slot#7640, gender#7596, category#7592, sum(amt#7593) AS amt#7647, count(1) AS count#7649L]\n            :  +- Filter isnotnull(event_time#7589)\n            :     +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0) + 300000000), LongType, TimestampType)) AS window#7650, event_time#7589, category#7592, amt#7593, gender#7596, is_fraud#7610]\n            :        +- Project [event_time#7589, category#7592, amt#7593, gender#7596, is_fraud#7610]\n            :           +- Project [splitted#7586[0] AS id#7588, cast(splitted#7586[1] as timestamp) AS event_time#7589, cast(splitted#7586[2] as bigint) AS cc_num#7590L, splitted#7586[3] AS merchant#7591, splitted#7586[4] AS category#7592, cast(splitted#7586[5] as double) AS amt#7593, splitted#7586[6] AS first#7594, splitted#7586[7] AS last#7595, splitted#7586[8] AS gender#7596, splitted#7586[9] AS street#7597, splitted#7586[10] AS city#7598, splitted#7586[11] AS state#7599, splitted#7586[12] AS zip#7600, splitted#7586[13] AS lat#7601, splitted#7586[14] AS long#7602, cast(splitted#7586[15] as bigint) AS city_pop#7603L, splitted#7586[16] AS job#7604, splitted#7586[17] AS dob#7605, splitted#7586[18] AS trans_num#7606, cast(splitted#7586[19] as bigint) AS unix_time#7607L, splitted#7586[20] AS merch_lat#7608, splitted#7586[21] AS merch_long#7609, cast(splitted#7586[22] as int) AS is_fraud#7610]\n            :              +- Project [split(value#7584, ,, -1) AS splitted#7586]\n            :                 +- Project [cast(value#7571 as string) AS value#7584]\n            :                    +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@54bde53b, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1d74aaca, [startingOffsets=earliest, kafka.bootstrap.servers=kafka1:9093, subscribe=records112556, failOnDataLoss=False], [key#7570, value#7571, topic#7572, partition#7573, offset#7574L, timestamp#7575, timestampType#7576], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@340524c,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9093, subscribe -> records112556, startingOffsets -> earliest, failOnDataLoss -> False),None), kafka, [key#7563, value#7564, topic#7565, partition#7566, offset#7567L, timestamp#7568, timestampType#7569]\n            +- ResolvedHint (strategy=broadcast)\n               +- Aggregate [time_slot#7640, gender#7596], [time_slot#7640 AS t_time_slot#7656, gender#7596 AS t_gender#7657, max(amt#7647) AS max_amt#7664]\n                  +- Aggregate [window#7650, gender#7596, category#7592], [window#7650 AS time_slot#7640, gender#7596, category#7592, sum(amt#7593) AS amt#7647, count(1) AS count#7649L]\n                     +- Filter isnotnull(event_time#7589)\n                        +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0) + 300000000), LongType, TimestampType)) AS window#7650, event_time#7589, category#7592, amt#7593, gender#7596, is_fraud#7610]\n                           +- Project [event_time#7589, category#7592, amt#7593, gender#7596, is_fraud#7610]\n                              +- Project [splitted#7586[0] AS id#7588, cast(splitted#7586[1] as timestamp) AS event_time#7589, cast(splitted#7586[2] as bigint) AS cc_num#7590L, splitted#7586[3] AS merchant#7591, splitted#7586[4] AS category#7592, cast(splitted#7586[5] as double) AS amt#7593, splitted#7586[6] AS first#7594, splitted#7586[7] AS last#7595, splitted#7586[8] AS gender#7596, splitted#7586[9] AS street#7597, splitted#7586[10] AS city#7598, splitted#7586[11] AS state#7599, splitted#7586[12] AS zip#7600, splitted#7586[13] AS lat#7601, splitted#7586[14] AS long#7602, cast(splitted#7586[15] as bigint) AS city_pop#7603L, splitted#7586[16] AS job#7604, splitted#7586[17] AS dob#7605, splitted#7586[18] AS trans_num#7606, cast(splitted#7586[19] as bigint) AS unix_time#7607L, splitted#7586[20] AS merch_lat#7608, splitted#7586[21] AS merch_long#7609, cast(splitted#7586[22] as int) AS is_fraud#7610]\n                                 +- Project [split(value#7584, ,, -1) AS splitted#7586]\n                                    +- Project [cast(value#7571 as string) AS value#7584]\n                                       +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@54bde53b, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1d74aaca, [startingOffsets=earliest, kafka.bootstrap.servers=kafka1:9093, subscribe=records112556, failOnDataLoss=False], [key#7570, value#7571, topic#7572, partition#7573, offset#7574L, timestamp#7575, timestampType#7576], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@340524c,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9093, subscribe -> records112556, startingOffsets -> earliest, failOnDataLoss -> False),None), kafka, [key#7563, value#7564, topic#7565, partition#7566, offset#7567L, timestamp#7568, timestampType#7569]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-0528011e7ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Write to a sink - here, the output is memory (only for testing).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m activityQuery = sdf.select(sdf.time_slot.start.cast('string').alias('start_time'), \n\u001b[0m\u001b[1;32m     91\u001b[0m                            \u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_slot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'end_time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                            \u001b[0;34m'gender'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'category'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Multiple streaming aggregations are not supported with streaming DataFrames/Datasets;\nProject [cast(time_slot#7640.start as string) AS start_time#7702, cast(time_slot#7640.end as string) AS end_time#7703, gender#7596, category#7592, round(amt#7647, 2) AS amt#7704, count#7649L]\n+- Project [time_slot#7640, gender#7596, category#7592, amt#7647, count#7649L]\n   +- Project [time_slot#7640, gender#7596, category#7592, amt#7647, count#7649L, max_amt#7664]\n      +- Project [time_slot#7640, gender#7596, category#7592, amt#7647, count#7649L, t_time_slot#7656, max_amt#7664]\n         +- Join Inner, (((time_slot#7640 = t_time_slot#7656) AND (gender#7596 = t_gender#7657)) AND (amt#7647 = max_amt#7664))\n            :- Aggregate [window#7650, gender#7596, category#7592], [window#7650 AS time_slot#7640, gender#7596, category#7592, sum(amt#7593) AS amt#7647, count(1) AS count#7649L]\n            :  +- Filter isnotnull(event_time#7589)\n            :     +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0) + 300000000), LongType, TimestampType)) AS window#7650, event_time#7589, category#7592, amt#7593, gender#7596, is_fraud#7610]\n            :        +- Project [event_time#7589, category#7592, amt#7593, gender#7596, is_fraud#7610]\n            :           +- Project [splitted#7586[0] AS id#7588, cast(splitted#7586[1] as timestamp) AS event_time#7589, cast(splitted#7586[2] as bigint) AS cc_num#7590L, splitted#7586[3] AS merchant#7591, splitted#7586[4] AS category#7592, cast(splitted#7586[5] as double) AS amt#7593, splitted#7586[6] AS first#7594, splitted#7586[7] AS last#7595, splitted#7586[8] AS gender#7596, splitted#7586[9] AS street#7597, splitted#7586[10] AS city#7598, splitted#7586[11] AS state#7599, splitted#7586[12] AS zip#7600, splitted#7586[13] AS lat#7601, splitted#7586[14] AS long#7602, cast(splitted#7586[15] as bigint) AS city_pop#7603L, splitted#7586[16] AS job#7604, splitted#7586[17] AS dob#7605, splitted#7586[18] AS trans_num#7606, cast(splitted#7586[19] as bigint) AS unix_time#7607L, splitted#7586[20] AS merch_lat#7608, splitted#7586[21] AS merch_long#7609, cast(splitted#7586[22] as int) AS is_fraud#7610]\n            :              +- Project [split(value#7584, ,, -1) AS splitted#7586]\n            :                 +- Project [cast(value#7571 as string) AS value#7584]\n            :                    +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@54bde53b, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1d74aaca, [startingOffsets=earliest, kafka.bootstrap.servers=kafka1:9093, subscribe=records112556, failOnDataLoss=False], [key#7570, value#7571, topic#7572, partition#7573, offset#7574L, timestamp#7575, timestampType#7576], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@340524c,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9093, subscribe -> records112556, startingOffsets -> earliest, failOnDataLoss -> False),None), kafka, [key#7563, value#7564, topic#7565, partition#7566, offset#7567L, timestamp#7568, timestampType#7569]\n            +- ResolvedHint (strategy=broadcast)\n               +- Aggregate [time_slot#7640, gender#7596], [time_slot#7640 AS t_time_slot#7656, gender#7596 AS t_gender#7657, max(amt#7647) AS max_amt#7664]\n                  +- Aggregate [window#7650, gender#7596, category#7592], [window#7650 AS time_slot#7640, gender#7596, category#7592, sum(amt#7593) AS amt#7647, count(1) AS count#7649L]\n                     +- Filter isnotnull(event_time#7589)\n                        +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) as double) = (cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) THEN (CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(event_time#7589, TimestampType, LongType) - 0) as double) / cast(300000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 300000000) + 0) + 300000000), LongType, TimestampType)) AS window#7650, event_time#7589, category#7592, amt#7593, gender#7596, is_fraud#7610]\n                           +- Project [event_time#7589, category#7592, amt#7593, gender#7596, is_fraud#7610]\n                              +- Project [splitted#7586[0] AS id#7588, cast(splitted#7586[1] as timestamp) AS event_time#7589, cast(splitted#7586[2] as bigint) AS cc_num#7590L, splitted#7586[3] AS merchant#7591, splitted#7586[4] AS category#7592, cast(splitted#7586[5] as double) AS amt#7593, splitted#7586[6] AS first#7594, splitted#7586[7] AS last#7595, splitted#7586[8] AS gender#7596, splitted#7586[9] AS street#7597, splitted#7586[10] AS city#7598, splitted#7586[11] AS state#7599, splitted#7586[12] AS zip#7600, splitted#7586[13] AS lat#7601, splitted#7586[14] AS long#7602, cast(splitted#7586[15] as bigint) AS city_pop#7603L, splitted#7586[16] AS job#7604, splitted#7586[17] AS dob#7605, splitted#7586[18] AS trans_num#7606, cast(splitted#7586[19] as bigint) AS unix_time#7607L, splitted#7586[20] AS merch_lat#7608, splitted#7586[21] AS merch_long#7609, cast(splitted#7586[22] as int) AS is_fraud#7610]\n                                 +- Project [split(value#7584, ,, -1) AS splitted#7586]\n                                    +- Project [cast(value#7571 as string) AS value#7584]\n                                       +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@54bde53b, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1d74aaca, [startingOffsets=earliest, kafka.bootstrap.servers=kafka1:9093, subscribe=records112556, failOnDataLoss=False], [key#7570, value#7571, topic#7572, partition#7573, offset#7574L, timestamp#7575, timestampType#7576], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@340524c,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9093, subscribe -> records112556, startingOffsets -> earliest, failOnDataLoss -> False),None), kafka, [key#7563, value#7564, topic#7565, partition#7566, offset#7567L, timestamp#7568, timestampType#7569]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, lit, window, sum, count, round, max, avg, broadcast\n",
    "from pyspark.sql.window import Window\n",
    "from time import sleep\n",
    "from datetime import datetime \n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Assignment_2\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "date = datetime.now().strftime(\"%m%d%M\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "# Read the whole dataset as a batch\n",
    "df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "        .option(\"subscribe\", f\"records{date}\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"failOnDataLoss\", 'False') \\\n",
    "        .load()\n",
    "\n",
    "# split the value\n",
    "records = df.selectExpr(\"CAST(value AS STRING)\").select(split(col('value'), \",\").alias('splitted'))\n",
    "\n",
    "# define the schema in this way\n",
    "new_df =  records.selectExpr('splitted[0] as id', \n",
    "                             'cast(splitted[1] as timestamp) as event_time',\n",
    "                             'cast(splitted[2] as long) as cc_num',\n",
    "                             'splitted[3] as merchant',\n",
    "                             'splitted[4] as category',\n",
    "                             'cast(splitted[5] as double) as amt',\n",
    "                             'splitted[6] as first',\n",
    "                             'splitted[7] as last',\n",
    "                             'splitted[8] as gender',\n",
    "                             'splitted[9] as street',\n",
    "                             'splitted[10] as city',\n",
    "                             'splitted[11] as state',\n",
    "                             'splitted[12] as zip',\n",
    "                             'splitted[13] as lat',\n",
    "                             'splitted[14] as long',\n",
    "                             'cast(splitted[15] as long) as city_pop',\n",
    "                             'splitted[16] as job',\n",
    "                             'splitted[17] as dob',\n",
    "                             'splitted[18] as trans_num',\n",
    "                             'cast(splitted[19] as long) as unix_time',\n",
    "                             'splitted[20] as merch_lat',\n",
    "                             'splitted[21] as merch_long',\n",
    "                             'cast(splitted[22] as int) as is_fraud')\n",
    "\n",
    "new_df = new_df.select('event_time', 'category', 'amt', 'gender', 'is_fraud')\n",
    "\n",
    "# filter out the fraud\n",
    "#new_df = new_df.where(col('is_fraud') == 1)\n",
    "\n",
    "window_5min = window(col('event_time'), '5 minutes').alias('time_slot')\n",
    "sdf = new_df.groupBy(window_5min, 'gender', 'category') \\\n",
    "            .agg(sum('amt').alias('amt'), count('*').alias('count'))\n",
    "\n",
    "################ Non-time-based windows are not supported on streaming DataFrames/Datasets #################\n",
    "#window_group_by = Window.partitionBy(window_5min, col('gender'))\n",
    "#sdf = sdf.withColumn('max_amt_per_5_min', max('amt').over(window_group_by)) \\\n",
    "#         .where(col('amt') == col('max_amt_per_5_min'))\\\n",
    "#         .drop('max_amt_per_5_min')\n",
    "\n",
    "############### Multiple streaming aggregations are not supported with streaming DataFrames/Datasets #############\n",
    "#temp_df = sdf.groupBy(col('time_slot').alias('t_time_slot'), \n",
    "#                      col(\"gender\").alias(\"t_gender\"))\\\n",
    "#             .agg(max('amt').alias('max_amt'))\n",
    "#join_exp = (sdf['time_slot'] == temp_df['t_time_slot']) & (sdf['gender'] == temp_df['t_gender']) & (sdf['amt'] == temp_df['max_amt'])\n",
    "#sdf = sdf.join(broadcast(temp_df), join_exp)\\\n",
    "#         .drop('t_gender')\\\n",
    "#         .drop('t_time_slot')\\\n",
    "#         .drop('max_amt')\n",
    "\n",
    "################ The sub-query in the where clause didn't seem to work #######################\n",
    "#sdf.createOrReplaceTempView('sdf_table')\n",
    "#sql = \"\"\"\n",
    "#    SELECT\n",
    "#       *\n",
    "#    FROM\n",
    "#        sdf_table\n",
    "#    WHERE   (time_slot, gender, amt) IN (\n",
    "#                SELECT time_slot, gender, MAX(amt) max_amt\n",
    "#                FROM sdf_table\n",
    "#                GROUP BY time_slot, gender )\n",
    "#\"\"\"\n",
    "#\n",
    "#sdf = spark.sql(sql)\n",
    "            \n",
    "# Write to a sink - here, the output is memory (only for testing). \n",
    "activityQuery = sdf.select(sdf.time_slot.start.cast('string').alias('start_time'), \n",
    "                           sdf.time_slot.end.cast('string').alias('end_time'), \n",
    "                           'gender', 'category',\n",
    "                           round('amt', 2).alias('amt'), 'count') \\\n",
    "                   .writeStream \\\n",
    "                   .queryName(\"most_fraud_table\") \\\n",
    "                   .format(\"memory\").outputMode(\"complete\") \\\n",
    "                   .start()\n",
    "# Testing \n",
    "for x in range(10):\n",
    "    spark.sql(\"SELECT * FROM most_fraud_table\").show(truncate=False)\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acdad09-753c-485c-8c87-b303392053ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = string_sdf_User_Team_avg_window \\\n",
    "              .select(concat(col(\"window_slot\"), lit(\",\"), col(\"User\"), lit(\",\"), col(\"Team\")).alias(\"key\"), \n",
    "                       col(\"avg_score\").alias('value') ) \\\n",
    "              .writeStream \\\n",
    "              .format(\"kafka\") \\\n",
    "              .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "              .option(\"checkpointLocation\", \"/home/jovyan/checkpoint\")\\\n",
    "              .option(\"topic\", \"avg_score\") \\\n",
    "              .outputMode(\"complete\") \\\n",
    "              .start()\n",
    "\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af9584fd-4cf0-4caa-a571-ec671dbd3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0346deed-8f8c-45bf-a8ca-8f4ac432669d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, TimestampType\n",
    "from time import sleep\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Lab7_exercise\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "dataSchema = StructType(\n",
    "    [StructField(\"User\", StringType(), True),\n",
    "     StructField(\"Team\", StringType(), True),\n",
    "     StructField(\"Score\", DoubleType(), True),\n",
    "     StructField(\"timestamp_in_ms\", LongType(), True),\n",
    "     StructField(\"event_time\", TimestampType(), True)\n",
    "     ])\n",
    "\n",
    "df = spark.read.format('csv').schema(dataSchema).option('header', 'False').load(\"/home/jovyan/data/gamestream/game_data_split1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83aca9fc-8e73-4600-b8b4-48859ef63500",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Score: double (nullable = true)\n",
      " |-- timestamp_in_ms: long (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n",
      "+--------------------------+--------------------+-----+---------------+-----------------------+\n",
      "|User                      |Team                |Score|timestamp_in_ms|event_time             |\n",
      "+--------------------------+--------------------+-----+---------------+-----------------------+\n",
      "|user16_AmaranthKoala      |AmaranthKoala       |18.0 |1447719060000  |2015-11-16 16:11:03.921|\n",
      "|user10_AndroidGreenKoala  |AndroidGreenKoala   |2.0  |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user9_AuburnCockatoo      |AuburnCockatoo      |5.0  |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user1_AntiqueBrassPlatypus|AntiqueBrassPlatypus|7.0  |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user9_BattleshipGreyPossum|BattleshipGreyPossum|14.0 |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user1_AmaranthDingo       |AmaranthDingo       |14.0 |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user10_ApricotDingo       |ApricotDingo        |16.0 |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user0_AzureBilby          |AzureBilby          |8.0  |1447719060000  |2015-11-16 16:11:03.955|\n",
      "|user9_AzureBilby          |AzureBilby          |5.0  |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user12_AmaranthMarmot     |AmaranthMarmot      |16.0 |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user15_AmaranthKoala      |AmaranthKoala       |7.0  |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user18_AndroidGreenKoala  |AndroidGreenKoala   |5.0  |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user7_BananaWallaby       |BananaWallaby       |15.0 |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user11_AmaranthKoala      |AmaranthKoala       |16.0 |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user17_AuburnDingo        |AuburnDingo         |12.0 |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user14_AzureBilby         |AzureBilby          |12.0 |1447719060000  |2015-11-16 16:11:03.959|\n",
      "|user5_AzureBilby          |AzureBilby          |18.0 |1447719060000  |2015-11-16 16:11:03.96 |\n",
      "|user17_ApricotDingo       |ApricotDingo        |19.0 |1447719060000  |2015-11-16 16:11:03.96 |\n",
      "|user4_AuburnCockatoo      |AuburnCockatoo      |2.0  |1447719060000  |2015-11-16 16:11:03.96 |\n",
      "|user9_AmaranthKoala       |AmaranthKoala       |5.0  |1447719060000  |2015-11-16 16:11:03.96 |\n",
      "+--------------------------+--------------------+-----+---------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
